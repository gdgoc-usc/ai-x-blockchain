{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Prediction Model using IRIS Dataset\n",
    "\n",
    "This notebook demonstrates a basic Multi-Layer Perceptron (MLP) with three layers using the IRIS dataset for classification. The model uses no activation functions (linear layers only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IRIS dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Target names: {iris.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MLP Model (3 Layers, No Activation Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"Multi-Layer Perceptron with 3 layers and no activation functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, seed=42):\n",
    "        \"\"\"Initialize the MLP with random weights using Xavier initialization.\"\"\"\n",
    "        rng = np.random.RandomState(seed)\n",
    "        \n",
    "        # Layer 1: Input -> Hidden1 (Xavier initialization)\n",
    "        self.W1 = rng.randn(input_size, hidden1_size) * np.sqrt(2.0 / (input_size + hidden1_size))\n",
    "        self.b1 = np.zeros((1, hidden1_size))\n",
    "        \n",
    "        # Layer 2: Hidden1 -> Hidden2 (Xavier initialization)\n",
    "        self.W2 = rng.randn(hidden1_size, hidden2_size) * np.sqrt(2.0 / (hidden1_size + hidden2_size))\n",
    "        self.b2 = np.zeros((1, hidden2_size))\n",
    "        \n",
    "        # Layer 3: Hidden2 -> Output (Xavier initialization)\n",
    "        self.W3 = rng.randn(hidden2_size, output_size) * np.sqrt(2.0 / (hidden2_size + output_size))\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through all layers (no activation functions).\"\"\"\n",
    "        # Layer 1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        \n",
    "        # Layer 2\n",
    "        self.z2 = np.dot(self.z1, self.W2) + self.b2\n",
    "        \n",
    "        # Layer 3 (Output)\n",
    "        self.z3 = np.dot(self.z2, self.W3) + self.b3\n",
    "        \n",
    "        # Softmax for output probabilities\n",
    "        exp_z3 = np.exp(self.z3 - np.max(self.z3, axis=1, keepdims=True))\n",
    "        self.output = exp_z3 / np.sum(exp_z3, axis=1, keepdims=True)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, X, y, learning_rate):\n",
    "        \"\"\"Backward pass to compute gradients and update weights.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dz3 = self.output - y\n",
    "        dW3 = np.dot(self.z2.T, dz3) / m\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer 2 gradient\n",
    "        dz2 = np.dot(dz3, self.W3.T)\n",
    "        dW2 = np.dot(self.z1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer 1 gradient\n",
    "        dz1 = np.dot(dz2, self.W2.T)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.b3 -= learning_rate * db3\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate, verbose=True):\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y, learning_rate)\n",
    "            \n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MLP with 3 layers\n",
    "# Input: 4 features, Hidden1: 8 neurons, Hidden2: 6 neurons, Output: 3 classes\n",
    "model = MLP(input_size=4, hidden1_size=8, hidden2_size=6, output_size=3)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the MLP...\\n\")\n",
    "losses = model.train(X_train, y_train_encoded, epochs=1000, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented a 3-layer MLP without activation functions for the IRIS classification task:\n",
    "\n",
    "- **Layer 1**: Input (4 features) → Hidden (8 neurons)\n",
    "- **Layer 2**: Hidden (8 neurons) → Hidden (6 neurons)\n",
    "- **Layer 3**: Hidden (6 neurons) → Output (3 classes)\n",
    "\n",
    "Note: Without activation functions between layers, the network is essentially a linear model, as the composition of linear transformations is still linear. However, the softmax function is applied at the output for probability distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
